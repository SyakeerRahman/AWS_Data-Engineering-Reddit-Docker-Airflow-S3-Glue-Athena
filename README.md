# AWS_Data-Engineering-Reddit-Docker-Airflow-S3-Glue-Athena

Reddit Data Pipeline AWS Cloud Data Engineering Project | Data Pipeline using Apache Hudi, EMR, Glue Pyspark, Redshift, Athena, Manage Airflow

I want to share my latest Data Engineering project where I put my skills to the test by working with an AWS cloud

ðŸ”¬ð—£ð—¿ð—¼ð—·ð—²ð—°ð˜ ð—¢ð˜ƒð—²ð—¿ð˜ƒð—¶ð—²ð˜„: This project provides a comprehensive data pipeline solution to extract, transform, and load (ETL) Reddit data into a Redshift data warehouse. The pipeline leverages a combination of tools and services including Apache Airflow, Celery, PostgreSQL, Amazon S3, AWS Glue, Amazon Athena, and Amazon Redshift.

ðŸ’¾ ð——ð—®ð˜ð—® ð—¦ð—¼ð˜‚ð—¿ð—°ð—²:

ðŸŽ¯ ð—£ð—¿ð—¼ð—·ð—²ð—°ð˜ ð—šð—¼ð—®ð—¹ð˜€:

Extract data from Reddit using its API.
Store the raw data into an S3 bucket from Airflow.
Transform the data using AWS Glue and Amazon Athena.
Load the transformed data into Amazon Redshift for analytics and querying.

ðŸ”§The tools that are covered in this project are:

Reddit API: Source of the data.
Apache Airflow & Celery: Orchestrates the ETL process and manages task distribution.
PostgreSQL: Temporary storage and metadata management.
Amazon S3: Raw data storage.
AWS Glue: Data cataloging and ETL jobs.
Amazon Athena: SQL-based data transformation.
Amazon Redshift: Data warehousing and analytics.


## Prerequisites

AWS Account with appropriate permissions for S3, Glue, Athena, and Redshift.
Reddit API credentials.
Docker Installation
Python 3.9 or higher



python -m venv venv
.\venv\Scripts\Activate.ps1
pip install apache-airflow pandas numpy praw
mkdir "config", "dags", "data", "etls", "logs", "pipelines", "tests", "utils"
"airflow.env", "docker-compose.yml", "Dockerfile" | ForEach-Object { New-Item $_ -ItemType File }
after paste the 
Install the dependencies. pip install -r requirements.txt
pip freeze > requirements.txt
docker compose up -d --build




